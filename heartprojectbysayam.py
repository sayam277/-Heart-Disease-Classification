# -*- coding: utf-8 -*-
"""HeartprojectbySayam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vDKDZrFASWKhkL6D0WkbDiEck9Uq8tm_
"""

import numpy as np

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('heart.csv')

df

df.info()

df.describe()

df.isnull().sum()

df.hist(figsize=(10,10))
plt.show()

sns.countplot(df.target, palette="icefire" )
plt.show()
df.target.value_counts(normalize=True)

continuous_cols = [col for col in df.columns if df[col].nunique()>15]
print(continuous_cols)

matrix = df[continuous_cols].corr()
mask = np.triu(np.ones_like(matrix, dtype=bool))
sns.heatmap(matrix,mask=mask,annot=True)

from scipy.stats import chi2_contingency

cat_cols = [col for col in df.columns if df[col].nunique()<=15]
print(cat_cols)
cat_cols.remove("target")
print(cat_cols)

def chitest(indepentVal, dependentVal):
    Crosstable = pd.crosstab(index=indepentVal,columns=dependentVal)
    ChiSqResult = chi2_contingency(Crosstable)
    return round(ChiSqResult[1],4)

chitest_dic = {}
for i in cat_cols:
 chitest_dic[i] = chitest(df[i], df['target'])
chitest_dic

fig,axes = plt.subplots(nrows=2,ncols=4, figsize=(12,8))
pd.crosstab(df.ca, df.target).plot(kind="bar",ax=axes[0, 0])
pd.crosstab(df.cp, df.target).plot(kind="bar",ax=axes[0, 1])
pd.crosstab(df.exang, df.target).plot(kind="bar",ax=axes[0, 2])
pd.crosstab(df.restecg, df.target).plot(kind="bar",ax=axes[0, 3])
pd.crosstab(df.sex, df.target).plot(kind="bar",ax=axes[1, 0])
pd.crosstab(df.slope, df.target).plot(kind="bar",ax=axes[1,1])
pd.crosstab(df.thal, df.target).plot(kind="bar",ax=axes[1, 2])
plt.show()

from sklearn.tree import DecisionTreeClassifier

from sklearn.pipeline import make_pipeline

from sklearn.model_selection import train_test_split,GridSearchCV

X= df.drop("target", axis=1)
y = df.target

from sklearn.compose import make_column_transformer,ColumnTransformer

from sklearn.preprocessing import OneHotEncoder , LabelEncoder , StandardScaler

ohe_col = []
for col in df.columns:
    if df[col].nunique() < 15:
        ohe_col.append(col)
        print("{}{}".format(col, df[col].unique()))
ohe_col.remove("target")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

ct = make_column_transformer((OneHotEncoder(handle_unknown='ignore'),ohe_col), remainder='passthrough')
sc = StandardScaler()

print(X_train.shape, X_test.shape)

# DECISION TREE

model = DecisionTreeClassifier(random_state=42)
pipe = make_pipeline(ct, model)
pipe.get_params().keys()
params = {'decisiontreeclassifier__splitter': ["best", "random"],
          'decisiontreeclassifier__max_depth': np.arange(1,10,1),
          'decisiontreeclassifier__min_samples_leaf': np.arange(1,5,1),
          'decisiontreeclassifier__min_samples_split': np.arange(2,5,1)
         }
cv_dt = GridSearchCV(pipe, param_grid=params, scoring="accuracy", n_jobs=-1)
cv_dt.fit(X_train, y_train)
preds_dt = cv_dt.predict(X_test)
print(cv_dt.best_params_, cv_dt.best_score_)
print(cv_dt.score(X_test, y_test))

#print_score(cv_dt, X_train, y_train, X_test, y_test)

# RANDOM FOREST

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(random_state=42)
pipe = make_pipeline(ct, model)
pipe.get_params().keys()
params = {'randomforestclassifier__n_estimators': np.arange(10,50,10),
         'randomforestclassifier__max_depth': np.arange(1,8,2)
         }
cv_rf = GridSearchCV(pipe, param_grid=params, scoring="accuracy", n_jobs=-1, cv=5)
cv_rf.fit(X_train, y_train)
preds_rf = cv_rf.predict(X_test)
print(cv_rf.best_params_, cv_rf.best_score_)
print(cv_rf.score(X_test, y_test))

# LOGISTIC REGRESSION

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='liblinear', random_state=42)
pipe = make_pipeline(ct, sc, model)
#pipe.get_params().keys()
params = {'logisticregression__penalty': ['l1','l2'], 'logisticregression__C': np.arange(0.1,1,0.1)}
cv_lr = GridSearchCV(pipe, param_grid=params, scoring="accuracy", n_jobs=-1)
cv_lr.fit(X_train, y_train)
preds_lr = cv_lr.predict(X_test)
print(cv_lr.best_params_, cv_lr.best_score_)
print(cv_lr.score(X_test, y_test))

#SVM

from sklearn.svm import SVC

model = SVC(random_state=42)
pipe = make_pipeline(ct, sc, model)
pipe.get_params().keys()
params = {'svc__kernel': ['linear', 'rbf'], 'svc__C': np.arange(0.5,1,0.1), 'svc__gamma': np.arange(0.01,0.05,0.01)}
cv_svm = GridSearchCV(pipe, param_grid=params, scoring="accuracy", n_jobs=-1)
cv_svm.fit(X_train, y_train)
preds_svm = cv_svm.predict(X_test)
print(cv_svm.best_params_, cv_svm.best_score_)
print(cv_svm.score(X_test, y_test))

# K NEIGHBORS

from sklearn.neighbors import KNeighborsClassifier

model = KN, sc, model)
pipe.get_params().keys()eighborsClassifier()
pipe = make_pipeline(ct
params = {'kneighborsclassifier__n_neighbors': np.arange(5,10,1), "kneighborsclassifier__weights": ['uniform', 'distance']}
cv_knn = GridSearchCV(pipe, param_grid=params, scoring="accuracy", n_jobs=-1)
cv_knn.fit(X_train, y_train)
preds_knn = cv_knn.predict(X_test)
print(cv_knn.best_params_, cv_knn.best_score_)
print(cv_knn.score(X_test, y_test))